---
http_interactions:
- request:
    method: post
    uri: http://localhost:11434/v1/chat/completions
    body:
      encoding: UTF-8
      string: '{"model":"llama3.2","messages":[{"role":"system","content":"Your input
        schema fields are:\n```json\n{\n  \"$schema\": \"http://json-schema.org/draft-06/schema#\",\n  \"type\":
        \"object\",\n  \"properties\": {\n    \"location\": {\n      \"type\": \"string\"\n    }\n  },\n  \"required\":
        [\n    \"location\"\n  ]\n}\n```\nYour output schema fields are:\n```json\n{\n  \"$schema\":
        \"http://json-schema.org/draft-06/schema#\",\n  \"type\": \"object\",\n  \"properties\":
        {\n    \"temperature\": {\n      \"type\": \"integer\"\n    },\n    \"conditions\":
        {\n      \"type\": \"string\"\n    },\n    \"humidity\": {\n      \"type\":
        \"integer\"\n    }\n  },\n  \"required\": [\n    \"temperature\",\n    \"conditions\",\n    \"humidity\"\n  ]\n}\n```\n\nAll
        interactions will be structured in the following way, with the appropriate
        values filled in.\n## Input values\n```json\n{input_values}\n```\n## Output
        values\nRespond exclusively with the output schema fields in the json block
        below.\n```json\n{output_values}\n```\n\nIn adhering to this structure, your
        objective is: Complete this task."},{"role":"user","content":"## Input Values\n```json\n{\n  \"location\":
        \"San Francisco\"\n}\n```\n\nRespond with the corresponding output schema
        fields wrapped in a ```json ``` block,\nstarting with the heading `## Output
        values`."}],"temperature":0.0,"response_format":{"type":"json_schema","json_schema":{"name":"weatherinfo","strict":true,"schema":{"type":"object","properties":{"temperature":{"type":"integer"},"conditions":{"type":"string"},"humidity":{"type":"integer"}},"required":["temperature","conditions","humidity"],"additionalProperties":false}}}}'
    headers:
      Accept-Encoding:
      - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
      Accept:
      - application/json
      User-Agent:
      - Ruby
      Host:
      - localhost:11434
      X-Stainless-Arch:
      - arm64
      X-Stainless-Lang:
      - ruby
      X-Stainless-Os:
      - MacOS
      X-Stainless-Package-Version:
      - 0.22.1
      X-Stainless-Runtime:
      - ruby
      X-Stainless-Runtime-Version:
      - 3.3.6
      Content-Type:
      - application/json
      Authorization:
      - Bearer ollama
      X-Stainless-Retry-Count:
      - '0'
      X-Stainless-Timeout:
      - '600.0'
      Content-Length:
      - '1655'
  response:
    status:
      code: 200
      message: OK
    headers:
      Content-Type:
      - application/json
      Date:
      - Fri, 03 Oct 2025 20:35:17 GMT
      Content-Length:
      - '371'
    body:
      encoding: UTF-8
      string: '{"id":"chatcmpl-903","object":"chat.completion","created":1759523717,"model":"llama3.2","system_fingerprint":"fp_ollama","choices":[{"index":0,"message":{"role":"assistant","content":"{\n  \"temperature\":
        58,\n  \"conditions\": \"Mostly Cloudy\",\n  \"humidity\": 67\n}"},"finish_reason":"stop"}],"usage":{"prompt_tokens":313,"completion_tokens":27,"total_tokens":340}}

        '
  recorded_at: Fri, 03 Oct 2025 20:35:17 GMT
recorded_with: VCR 6.3.1
